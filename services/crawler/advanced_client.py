"""
é«˜çº§çˆ¬è™«å®¢æˆ·ç«¯ - é›†æˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
ç»“åˆæ™ºèƒ½é‡è¯•ã€è¿æ¥æ± ã€å¥åº·æ¢æµ‹,å®ç°é«˜æˆåŠŸç‡çš„çˆ¬å–

ä½¿ç”¨ç¤ºä¾‹:
    client = AdvancedCrawlerClient("https://example.com")
    await client.initialize()
    
    data = await client.fetch("/api/data")
    
    await client.close()
"""

import asyncio
import aiohttp
import time
from typing import Optional, Dict, Any
import logging

from .smart_retry import SmartRetryManager, RetryConfig, RetryStrategy
from .connection_pool import SmartConnectionPool
from .health_prober import ServerHealthProber

log = logging.getLogger(__name__)


class AdvancedCrawlerClient:
    """é«˜çº§çˆ¬è™«å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        base_url: str,
        enable_connection_pool: bool = True,
        enable_health_probe: bool = True,
        enable_smart_retry: bool = True,
        pool_size: int = 5,
        max_retries: int = 10,
        user_agent: Optional[str] = None,
    ):
        """
        Args:
            base_url: åŸºç¡€URL(å¦‚ https://example.com)
            enable_connection_pool: æ˜¯å¦å¯ç”¨è¿æ¥æ± 
            enable_health_probe: æ˜¯å¦å¯ç”¨å¥åº·æ¢æµ‹
            enable_smart_retry: æ˜¯å¦å¯ç”¨æ™ºèƒ½é‡è¯•
            pool_size: è¿æ¥æ± å¤§å°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            user_agent: è‡ªå®šä¹‰User-Agent
        """
        self.base_url = base_url.rstrip('/')
        self.enable_connection_pool = enable_connection_pool
        self.enable_health_probe = enable_health_probe
        self.enable_smart_retry = enable_smart_retry
        
        # æå–åŸŸå
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        self.domain = parsed.netloc
        
        # ç»„ä»¶
        self.connection_pool: Optional[SmartConnectionPool] = None
        self.health_prober: Optional[ServerHealthProber] = None
        self.retry_manager: Optional[SmartRetryManager] = None
        
        # é…ç½®
        self.pool_size = pool_size
        self.max_retries = max_retries
        self.user_agent = user_agent or (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
        
        # ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "retries_count": 0,
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        log.info(f"åˆå§‹åŒ–é«˜çº§çˆ¬è™«å®¢æˆ·ç«¯: {self.base_url}")
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        if self.enable_connection_pool:
            self.connection_pool = SmartConnectionPool(
                target_url=self.base_url,
                pool_size=self.pool_size,
            )
            await self.connection_pool.initialize()
            log.info("âœ… è¿æ¥æ± å·²å¯ç”¨")
        
        # åˆå§‹åŒ–å¥åº·æ¢æµ‹
        if self.enable_health_probe:
            self.health_prober = ServerHealthProber(
                target_domain=self.domain,
                probe_interval=30.0,
            )
            await self.health_prober.start()
            log.info("âœ… å¥åº·æ¢æµ‹å·²å¯ç”¨")
        
        # åˆå§‹åŒ–é‡è¯•ç®¡ç†å™¨
        if self.enable_smart_retry:
            retry_config = RetryConfig(
                max_retries=self.max_retries,
                base_delay=1.0,
                max_delay=30.0,
                jitter_factor=0.5,
                give_up_probability=0.15,
            )
            self.retry_manager = SmartRetryManager(retry_config)
            log.info("âœ… æ™ºèƒ½é‡è¯•å·²å¯ç”¨")
        
        log.info("ğŸš€ é«˜çº§çˆ¬è™«å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        log.info("å…³é—­é«˜çº§çˆ¬è™«å®¢æˆ·ç«¯")
        
        if self.connection_pool:
            await self.connection_pool.close()
        
        if self.health_prober:
            await self.health_prober.stop()
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        log.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {self.get_stats()}")
    
    async def fetch(
        self,
        path: str,
        method: str = "GET",
        **kwargs
    ) -> str:
        """
        æŠ“å–æ•°æ®
        
        Args:
            path: è·¯å¾„(å¦‚ /api/data)
            method: HTTPæ–¹æ³•
            **kwargs: ä¼ é€’ç»™requestçš„å‚æ•°
        
        Returns:
            å“åº”æ–‡æœ¬
        """
        url = f"{self.base_url}{path}"
        
        # å‡†å¤‡è¯·æ±‚
        async def _do_request():
            return await self._execute_request(method, url, **kwargs)
        
        # ä½¿ç”¨æ™ºèƒ½é‡è¯•
        if self.enable_smart_retry and self.retry_manager:
            def on_retry(attempt, exception):
                self.stats["retries_count"] += 1
                log.debug(f"é‡è¯• {attempt}/{self.max_retries}: {exception}")
            
            result = await self.retry_manager.retry_with_strategy(
                _do_request,
                strategy=RetryStrategy.LUCKY_USER,
                on_retry=on_retry,
            )
        else:
            result = await _do_request()
        
        return result
    
    async def _execute_request(
        self,
        method: str,
        url: str,
        **kwargs
    ) -> str:
        """æ‰§è¡Œå•æ¬¡è¯·æ±‚"""
        self.stats["total_requests"] += 1
        
        # è®¾ç½®Headers
        headers = kwargs.get('headers', {})
        headers.setdefault('User-Agent', self.user_agent)
        kwargs['headers'] = headers
        
        # é€‰æ‹©æœ€ä½³èŠ‚ç‚¹(å¦‚æœå¯ç”¨å¥åº·æ¢æµ‹)
        target_ip = None
        if self.enable_health_probe and self.health_prober:
            best_node = self.health_prober.get_best_node()
            if best_node:
                target_ip = best_node.ip
                log.debug(f"ä½¿ç”¨æœ€ä½³èŠ‚ç‚¹: {target_ip} (å¥åº·åˆ†{best_node.health_score:.1f})")
        
        # ä½¿ç”¨è¿æ¥æ± å‘é€è¯·æ±‚
        if self.enable_connection_pool and self.connection_pool:
            async with await self.connection_pool.request(method, url, **kwargs) as resp:
                text = await resp.text()
                self.stats["successful_requests"] += 1
                return text
        
        # é™çº§: æ™®é€šè¯·æ±‚
        else:
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.request(method, url, **kwargs) as resp:
                    text = await resp.text()
                    self.stats["successful_requests"] += 1
                    return text
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        result = {**self.stats}
        
        # æˆåŠŸç‡
        if result["total_requests"] > 0:
            result["success_rate"] = (
                result["successful_requests"] / result["total_requests"]
            )
        else:
            result["success_rate"] = 0.0
        
        # è¿æ¥æ± ç»Ÿè®¡
        if self.connection_pool:
            result["connection_pool"] = self.connection_pool.get_stats()
        
        # å¥åº·æ¢æµ‹ç»Ÿè®¡
        if self.health_prober:
            result["health_prober"] = self.health_prober.get_stats()
        
        # é‡è¯•ç®¡ç†å™¨ç»Ÿè®¡
        if self.retry_manager:
            result["retry_manager"] = self.retry_manager.get_stats()
        
        return result


# ä¾¿æ·å‡½æ•°
async def fetch_with_advanced_client(
    url: str,
    **client_kwargs
) -> str:
    """
    ä½¿ç”¨é«˜çº§å®¢æˆ·ç«¯æŠ“å–å•ä¸ªURL
    
    ç”¨æ³•:
        data = await fetch_with_advanced_client(
            "https://example.com/api/data",
            max_retries=5
        )
    """
    from urllib.parse import urlparse
    
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    path = parsed.path or "/"
    if parsed.query:
        path += f"?{parsed.query}"
    
    client = AdvancedCrawlerClient(base_url, **client_kwargs)
    
    try:
        await client.initialize()
        result = await client.fetch(path)
        return result
    finally:
        await client.close()
