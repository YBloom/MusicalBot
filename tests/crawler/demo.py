"""
ç®€å•æ¼”ç¤º - å±•ç¤ºé«˜çº§çˆ¬è™«çš„å¨åŠ›
è¿è¡Œæ­¤è„šæœ¬å¯ä»¥å¿«é€Ÿçœ‹åˆ°æ•ˆæœ
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from services.crawler.advanced_client import AdvancedCrawlerClient
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
log = logging.getLogger(__name__)


async def demo():
    """æ¼”ç¤ºè„šæœ¬"""
    
    print("\n" + "="*60)
    print("ğŸ¯ é«˜çº§çˆ¬è™«æ¼”ç¤º")
    print("="*60)
    
    print("\nâš™ï¸  é…ç½®:")
    print("   - å¯ç”¨è¿æ¥æ± : âœ…")
    print("   - å¯ç”¨æ™ºèƒ½é‡è¯•: âœ…")
    print("   - å¯ç”¨å¥åº·æ¢æµ‹: âŒ (æœ¬åœ°æµ‹è¯•ä¸éœ€è¦)")
    print("   - æœ€å¤§é‡è¯•: 10æ¬¡")
    print("   - è¿æ¥æ± å¤§å°: 3")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = AdvancedCrawlerClient(
        base_url="http://localhost:8080",
        enable_connection_pool=True,
        enable_health_probe=False,
        enable_smart_retry=True,
        pool_size=3,
        max_retries=10,
    )
    
    print("\nğŸš€ åˆå§‹åŒ–å®¢æˆ·ç«¯...")
    await client.initialize()
    
    print("\nğŸ“¡ å¼€å§‹å‘é€è¯·æ±‚...")
    
    # å‘é€5æ¬¡è¯·æ±‚
    for i in range(1, 6):
        print(f"\nç¬¬ {i} æ¬¡è¯·æ±‚:")
        try:
            result = await client.fetch("/")
            print(f"   âœ… æˆåŠŸ!")
            
            # è§£æå“åº”
            import json
            try:
                data = json.loads(result)
                print(f"   ğŸ“Š æœåŠ¡å™¨ç»Ÿè®¡: æˆåŠŸ{data['stats']['success']}, "
                      f"æ‹’ç»{data['stats']['rejected']}, "
                      f"è¶…æ—¶{data['stats']['timeout']}")
            except:
                pass
        
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {type(e).__name__}")
    
    # è·å–ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š å®¢æˆ·ç«¯ç»Ÿè®¡")
    print("="*60)
    
    stats = client.get_stats()
    
    print(f"\næ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"æˆåŠŸè¯·æ±‚: {stats['successful_requests']}")
    print(f"å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")
    print(f"é‡è¯•æ¬¡æ•°: {stats['retries_count']}")
    
    if 'retry_manager' in stats:
        rm = stats['retry_manager']
        print(f"\næ™ºèƒ½é‡è¯•ç»Ÿè®¡:")
        print(f"   å½“å‰åŸºç¡€å»¶è¿Ÿ: {rm['current_base_delay']:.2f}s")
        print(f"   å½“å‰æ”¾å¼ƒæ¦‚ç‡: {rm['current_give_up_prob']*100:.1f}%")
        print(f"   è¿‘æœŸæˆåŠŸç‡: {rm['recent_success_rate']*100:.1f}%")
    
    if 'connection_pool' in stats:
        cp = stats['connection_pool']
        print(f"\nè¿æ¥æ± ç»Ÿè®¡:")
        print(f"   æ± å¤§å°: {cp['pool_size']}")
        print(f"   å¥åº·è¿æ¥: {cp['healthy_connections']}")
        print(f"   å¹³å‡å“åº”: {cp['avg_response_time']:.2f}s")
    
    # å…³é—­
    print("\nğŸ›‘ å…³é—­å®¢æˆ·ç«¯...")
    await client.close()
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("="*60 + "\n")


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘     é«˜è´Ÿè½½æœåŠ¡å™¨çˆ¬è™«è§£å†³æ–¹æ¡ˆ - æ¼”ç¤ºè„šæœ¬                  â•‘
â•‘                                                          â•‘
â•‘  ç¡®ä¿æµ‹è¯•æœåŠ¡å™¨å·²å¯åŠ¨:                                    â•‘
â•‘  python tests/crawler/test_server.py                    â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    asyncio.run(demo())
