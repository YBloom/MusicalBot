"""
æµ‹è¯•å®¢æˆ·ç«¯ - å¯¹æ¯”æ™®é€šçˆ¬è™« vs é«˜çº§çˆ¬è™«
éªŒè¯ä¼˜åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§
"""

import asyncio
import aiohttp
import time
import sys
import os
from typing import Dict, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from services.crawler.advanced_client import AdvancedCrawlerClient
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger(__name__)


class NaiveCrawler:
    """æ™®é€šçˆ¬è™«(æš´åŠ›é‡è¯•)"""
    
    def __init__(self, base_url: str, max_retries: int = 10):
        self.base_url = base_url
        self.max_retries = max_retries
        
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
        }
    
    async def fetch(self, path: str = "/") -> str:
        """æš´åŠ›æŠ“å–"""
        url = f"{self.base_url}{path}"
        
        for attempt in range(1, self.max_retries + 1):
            self.stats["total_requests"] += 1
            
            try:
                # ç®€å•ç²—æš´: ç«‹å³é‡è¯•
                timeout = aiohttp.ClientTimeout(total=5)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url) as resp:
                        text = await resp.text()
                        self.stats["successful_requests"] += 1
                        return text
            
            except Exception as e:
                if attempt == self.max_retries:
                    self.stats["failed_requests"] += 1
                    raise
                
                # ç«‹å³é‡è¯•(æ— å»¶è¿Ÿ)
                continue
        
        raise Exception("Failed after all retries")


async def benchmark_naive_crawler(target_url: str, num_requests: int = 10):
    """åŸºå‡†æµ‹è¯•: æ™®é€šçˆ¬è™«"""
    log.info("="*60)
    log.info("ðŸ”´ åŸºå‡†æµ‹è¯•: æ™®é€šçˆ¬è™«(æš´åŠ›é‡è¯•)")
    log.info("="*60)
    
    crawler = NaiveCrawler(target_url, max_retries=10)
    
    success_count = 0
    failed_count = 0
    total_time = 0
    
    for i in range(num_requests):
        start = time.time()
        try:
            result = await crawler.fetch()
            success_count += 1
            elapsed = time.time() - start
            total_time += elapsed
            log.info(f"âœ… è¯·æ±‚ {i+1}/{num_requests} æˆåŠŸ (è€—æ—¶{elapsed:.2f}s)")
        
        except Exception as e:
            failed_count += 1
            elapsed = time.time() - start
            total_time += elapsed
            log.error(f"âŒ è¯·æ±‚ {i+1}/{num_requests} å¤±è´¥ (è€—æ—¶{elapsed:.2f}s)")
    
    # ç»Ÿè®¡
    success_rate = success_count / num_requests * 100 if num_requests > 0 else 0
    avg_time = total_time / num_requests if num_requests > 0 else 0
    
    log.info("")
    log.info("ðŸ“Š æ™®é€šçˆ¬è™«ç»Ÿè®¡:")
    log.info(f"   æˆåŠŸ: {success_count}/{num_requests} ({success_rate:.1f}%)")
    log.info(f"   å¤±è´¥: {failed_count}/{num_requests}")
    log.info(f"   å¹³å‡è€—æ—¶: {avg_time:.2f}s")
    log.info(f"   æ€»HTTPè¯·æ±‚: {crawler.stats['total_requests']}")
    log.info("")
    
    return {
        "success_rate": success_rate,
        "avg_time": avg_time,
        "total_http_requests": crawler.stats['total_requests'],
    }


async def benchmark_advanced_crawler(target_url: str, num_requests: int = 10):
    """åŸºå‡†æµ‹è¯•: é«˜çº§çˆ¬è™«"""
    log.info("="*60)
    log.info("ðŸŸ¢ åŸºå‡†æµ‹è¯•: é«˜çº§çˆ¬è™«(æ™ºèƒ½ç­–ç•¥)")
    log.info("="*60)
    
    client = AdvancedCrawlerClient(
        target_url,
        enable_connection_pool=True,
        enable_health_probe=False,  # æœ¬åœ°æµ‹è¯•ä¸éœ€è¦
        enable_smart_retry=True,
        pool_size=3,
        max_retries=10,
    )
    
    await client.initialize()
    
    success_count = 0
    failed_count = 0
    total_time = 0
    
    for i in range(num_requests):
        start = time.time()
        try:
            result = await client.fetch("/")
            success_count += 1
            elapsed = time.time() - start
            total_time += elapsed
            log.info(f"âœ… è¯·æ±‚ {i+1}/{num_requests} æˆåŠŸ (è€—æ—¶{elapsed:.2f}s)")
        
        except Exception as e:
            failed_count += 1
            elapsed = time.time() - start
            total_time += elapsed
            log.error(f"âŒ è¯·æ±‚ {i+1}/{num_requests} å¤±è´¥ (è€—æ—¶{elapsed:.2f}s): {e}")
    
    # ç»Ÿè®¡
    success_rate = success_count / num_requests * 100 if num_requests > 0 else 0
    avg_time = total_time / num_requests if num_requests > 0 else 0
    
    stats = client.get_stats()
    
    log.info("")
    log.info("ðŸ“Š é«˜çº§çˆ¬è™«ç»Ÿè®¡:")
    log.info(f"   æˆåŠŸ: {success_count}/{num_requests} ({success_rate:.1f}%)")
    log.info(f"   å¤±è´¥: {failed_count}/{num_requests}")
    log.info(f"   å¹³å‡è€—æ—¶: {avg_time:.2f}s")
    log.info(f"   æ€»HTTPè¯·æ±‚: {stats['total_requests']}")
    log.info(f"   é‡è¯•æ¬¡æ•°: {stats['retries_count']}")
    
    if 'retry_manager' in stats:
        rm_stats = stats['retry_manager']
        log.info(f"   æ™ºèƒ½é‡è¯•æˆåŠŸçŽ‡: {rm_stats['recent_success_rate']*100:.1f}%")
    
    if 'connection_pool' in stats:
        cp_stats = stats['connection_pool']
        log.info(f"   è¿žæŽ¥æ± ç»Ÿè®¡: {cp_stats}")
    
    log.info("")
    
    await client.close()
    
    return {
        "success_rate": success_rate,
        "avg_time": avg_time,
        "total_http_requests": stats['total_requests'],
        "retries": stats['retries_count'],
    }


async def run_comparison_test():
    """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
    # ç›®æ ‡URL(æµ‹è¯•æœåŠ¡å™¨)
    target_url = "http://localhost:8080"
    num_requests = 20  # æ¯ä¸ªå®¢æˆ·ç«¯å‘é€20æ¬¡è¯·æ±‚
    
    log.info("\n" + "ðŸŽ¯ å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
    log.info(f"ç›®æ ‡: {target_url}")
    log.info(f"æ¯ä¸ªå®¢æˆ·ç«¯è¯·æ±‚æ¬¡æ•°: {num_requests}\n")
    
    # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    log.info("ç­‰å¾…æµ‹è¯•æœåŠ¡å™¨å¯åŠ¨(5ç§’)...")
    await asyncio.sleep(5)
    
    # æµ‹è¯•1: æ™®é€šçˆ¬è™«
    naive_results = await benchmark_naive_crawler(target_url, num_requests)
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(3)
    
    # æµ‹è¯•2: é«˜çº§çˆ¬è™«
    advanced_results = await benchmark_advanced_crawler(target_url, num_requests)
    
    # å¯¹æ¯”åˆ†æž
    log.info("="*60)
    log.info("ðŸ“ˆ å¯¹æ¯”åˆ†æž")
    log.info("="*60)
    
    log.info(f"\næˆåŠŸçŽ‡å¯¹æ¯”:")
    log.info(f"  æ™®é€šçˆ¬è™«: {naive_results['success_rate']:.1f}%")
    log.info(f"  é«˜çº§çˆ¬è™«: {advanced_results['success_rate']:.1f}%")
    log.info(f"  æå‡: {advanced_results['success_rate'] - naive_results['success_rate']:.1f}%")
    
    log.info(f"\nå¹³å‡è€—æ—¶å¯¹æ¯”:")
    log.info(f"  æ™®é€šçˆ¬è™«: {naive_results['avg_time']:.2f}s")
    log.info(f"  é«˜çº§çˆ¬è™«: {advanced_results['avg_time']:.2f}s")
    
    log.info(f"\nHTTPè¯·æ±‚æ•°å¯¹æ¯”:")
    log.info(f"  æ™®é€šçˆ¬è™«: {naive_results['total_http_requests']} æ¬¡")
    log.info(f"  é«˜çº§çˆ¬è™«: {advanced_results['total_http_requests']} æ¬¡")
    
    if advanced_results['success_rate'] > naive_results['success_rate']:
        log.info(f"\nâœ… é«˜çº§çˆ¬è™«èƒœå‡º! æˆåŠŸçŽ‡æå‡ {advanced_results['success_rate'] - naive_results['success_rate']:.1f}%")
    else:
        log.info(f"\nâš ï¸  æµ‹è¯•ç»“æžœä¸ç†æƒ³,å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
    
    log.info("\n" + "="*60)


async def quick_test():
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªè¯·æ±‚"""
    target_url = "http://localhost:8080"
    
    log.info("å¿«é€Ÿæµ‹è¯•: å‘é€å•ä¸ªè¯·æ±‚")
    
    client = AdvancedCrawlerClient(
        target_url,
        enable_connection_pool=True,
        enable_health_probe=False,
        enable_smart_retry=True,
        max_retries=5,
    )
    
    await client.initialize()
    
    try:
        result = await client.fetch("/")
        log.info(f"âœ… æˆåŠŸ! å“åº”: {result[:200]}...")
    except Exception as e:
        log.error(f"âŒ å¤±è´¥: {e}")
    
    stats = client.get_stats()
    log.info(f"ç»Ÿè®¡: {stats}")
    
    await client.close()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "quick":
        # å¿«é€Ÿæµ‹è¯•
        asyncio.run(quick_test())
    else:
        # å®Œæ•´å¯¹æ¯”æµ‹è¯•
        asyncio.run(run_comparison_test())
